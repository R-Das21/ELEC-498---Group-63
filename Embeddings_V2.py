import os
import sys
from openai import OpenAI
import numpy as np
import pandas as pd
from tqdm import tqdm
import tiktoken

# Path to the CSV generated by your parsing script
INPUT_CSV = "C:/Users/rishi/OneDrive/Desktop/ELEC498 Project FIles/citeulike-t-master/citeulike-t-master/papers.csv"

# Local output file where embeddings will be saved
EMBEDDINGS_OUT = "C:/Users/rishi/OneDrive/Desktop/ELEC498 Project FIles/citeulike-t-master/citeulike-t-master/paper_embeddings.npy"

# OpenAI embedding model
EMBEDDING_MODEL = "text-embedding-ada-002"
MAX_TOKENS = 8191  # Maximum tokens for text-embedding-ada-002

def num_tokens(text: str, model: str = EMBEDDING_MODEL) -> int:
    """Return the number of tokens in a string."""
    encoding = tiktoken.encoding_for_model(model)
    return len(encoding.encode(text))

def truncate_text(text: str, model: str = EMBEDDING_MODEL) -> str:
    """Truncate text to fit within token limit."""
    encoding = tiktoken.encoding_for_model(model)
    encoded = encoding.encode(text)
    if len(encoded) > MAX_TOKENS:
        truncated = encoded[:MAX_TOKENS]
        return encoding.decode(truncated)
    return text

def create_embeddings(client, texts, batch_size=100):
    """
    Create embeddings for a list of texts in batches.
    
    Args:
        client: OpenAI client instance
        texts: List of texts to embed
        batch_size: Number of texts to process in each batch
        
    Returns:
        List of embeddings
    """
    all_embeddings = []
    
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        # Truncate each text in the batch
        truncated_batch = [truncate_text(text) for text in batch]
        
        try:
            response = client.embeddings.create(
                model=EMBEDDING_MODEL,
                input=truncated_batch
            )
            # Extract embeddings from response
            batch_embeddings = [item.embedding for item in response.data]
            all_embeddings.extend(batch_embeddings)
            
        except Exception as e:
            print(f"[ERROR] Batch embedding failed: {str(e)}")
            # Fill failed embeddings with zeros
            batch_embeddings = [np.zeros(1536) for _ in batch]
            all_embeddings.extend(batch_embeddings)
            
    return all_embeddings

def process_long_text(client, text: str, model: str = EMBEDDING_MODEL) -> np.ndarray:
    """
    Process long text by splitting into chunks and averaging embeddings.
    
    Args:
        client: OpenAI client instance
        text: The text to process
        model: The embedding model to use
        
    Returns:
        numpy array of averaged embeddings
    """
    encoding = tiktoken.encoding_for_model(model)
    tokens = encoding.encode(text)
    
    # Split into chunks of MAX_TOKENS
    chunks = []
    for i in range(0, len(tokens), MAX_TOKENS):
        chunk = tokens[i:i + MAX_TOKENS]
        chunks.append(encoding.decode(chunk))
    
    # Get embeddings for each chunk
    chunk_embeddings = []
    for chunk in chunks:
        try:
            response = client.embeddings.create(
                model=EMBEDDING_MODEL,
                input=chunk
            )
            embedding = response.data[0].embedding
            chunk_embeddings.append(embedding)
        except Exception as e:
            print(f"[ERROR] Chunk embedding failed: {str(e)}")
            continue
    
    if not chunk_embeddings:
        return np.zeros(1536)
    
    # Average the embeddings
    return np.mean(chunk_embeddings, axis=0)

def main():
    try:
        # Initialize OpenAI client
        client = OpenAI(api_key= '')
        
        # Load the CSV with paper IDs and text
        if not os.path.exists(INPUT_CSV):
            print(f"Error: CSV file not found at {INPUT_CSV}")
            sys.exit(1)

        df = pd.read_csv(INPUT_CSV)
        print(f"[INFO] Loaded {len(df)} papers from {INPUT_CSV}")

        # Initialize array for embeddings
        all_embeddings = np.zeros((len(df), 1536), dtype=np.float32)
        
        # Process each text with progress bar
        print(f"[INFO] Generating embeddings using OpenAI model: {EMBEDDING_MODEL}")
        for i, row in tqdm(df.iterrows(), total=len(df), desc="Processing papers"):
            text = str(row['text'])
            tokens = num_tokens(text)
            
            if tokens <= MAX_TOKENS:
                # For texts within token limit, process in batch
                try:
                    response = client.embeddings.create(
                        model=EMBEDDING_MODEL,
                        input=truncate_text(text)
                    )
                    all_embeddings[i] = response.data[0].embedding
                except Exception as e:
                    print(f"[WARNING] Failed to embed paper_id={row.get('paper_id', i)}: {e}")
                    continue
            else:
                # For texts exceeding token limit, process in chunks
                print(f"[INFO] Processing long text ({tokens} tokens) for paper_id={row.get('paper_id', i)}")
                all_embeddings[i] = process_long_text(client, text)  # Added client parameter here

        # Save the embeddings to a .npy file
        print(f"[INFO] Saving embeddings to {EMBEDDINGS_OUT}")
        np.save(EMBEDDINGS_OUT, all_embeddings)

        print("[INFO] Embedding creation complete!")
        
    except Exception as e:
        print(f"[ERROR] An unexpected error occurred: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    main()